{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidsvash26/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#Inspired from script on forum - https://www.kaggle.com/dasolmar/xgb-with-whq-jaccard \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "import operator\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import plot, show, subplot, specgram, imshow, savefig\n",
    "\n",
    "RS = 12357\n",
    "ROUNDS = 315\n",
    "\n",
    "print(\"Started\")\n",
    "np.random.seed(RS)\n",
    "input_folder = '/home/sidsvash26/kaggle_quora/data/'\n",
    "\n",
    "def train_xgb(X, y, params):\n",
    "\tprint(\"Will train XGB for {} rounds, RandomSeed: {}\".format(ROUNDS, RS))\n",
    "\tx, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n",
    "\n",
    "\txg_train = xgb.DMatrix(x, label=y_train)\n",
    "\txg_val = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "\twatchlist  = [(xg_train,'train'), (xg_val,'eval')]\n",
    "\treturn xgb.train(params, xg_train, ROUNDS, watchlist)\n",
    "\n",
    "def predict_xgb(clr, X_test):\n",
    "\treturn clr.predict(xgb.DMatrix(X_test))\n",
    "\n",
    "def create_feature_map(features):\n",
    "\toutfile = open('xgb.fmap', 'w')\n",
    "\ti = 0\n",
    "\tfor feat in features:\n",
    "\t\toutfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "\t\ti = i + 1\n",
    "\toutfile.close()\n",
    "\n",
    "def add_word_count(x, df, word):\n",
    "\tx['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n",
    "\tx['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n",
    "\tx[word + '_both'] = x['q1_' + word] * x['q2_' + word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: X_train: (404290, 6), X_test: (2345796, 3)\n",
      "Features processing, be patient...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidsvash26/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sidsvash26/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:80: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/sidsvash26/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:75: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25% done....\n",
      "50% done....\n",
      "75% done....\n",
      "100% done....\n",
      "Index(['word_match', 'word_match_2root', 'tfidf_word_match', 'shared_count',\n",
      "       'stops1_ratio', 'stops2_ratio', 'shared_2gram', 'cosine',\n",
      "       'words_hamming', 'diff_stops_r', 'len_q1', 'len_q2', 'diff_len',\n",
      "       'caps_count_q1', 'caps_count_q2', 'diff_caps', 'len_char_q1',\n",
      "       'len_char_q2', 'diff_len_char', 'len_word_q1', 'len_word_q2',\n",
      "       'diff_len_word', 'avg_world_len1', 'avg_world_len2', 'diff_avg_word',\n",
      "       'exactly_same', 'duplicated', 'q1_how', 'q2_how', 'how_both', 'q1_what',\n",
      "       'q2_what', 'what_both', 'q1_which', 'q2_which', 'which_both', 'q1_who',\n",
      "       'q2_who', 'who_both', 'q1_where', 'q2_where', 'where_both', 'q1_when',\n",
      "       'q2_when', 'when_both', 'q1_why', 'q2_why', 'why_both'],\n",
      "      dtype='object')\n",
      "         word_match  word_match_2root  tfidf_word_match  shared_count  \\\n",
      "count  2.744371e+06      2.744371e+06      2.750086e+06  2.750086e+06   \n",
      "mean   1.542752e-01      3.228801e-01      2.016003e-01  1.605620e+00   \n",
      "std    1.375446e-01      2.236598e-01      2.011869e-01  1.548127e+00   \n",
      "min    0.000000e+00      0.000000e+00      0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00      0.000000e+00      0.000000e+00  0.000000e+00   \n",
      "50%    1.355803e-01      3.682123e-01      1.666667e-01  1.000000e+00   \n",
      "75%    2.456540e-01      4.956349e-01      2.857143e-01  2.000000e+00   \n",
      "max    5.000000e-01      7.071068e-01      1.000000e+00  3.200000e+01   \n",
      "\n",
      "       stops1_ratio  stops2_ratio  shared_2gram        cosine  words_hamming  \\\n",
      "count  2.750086e+06  2.750086e+06  2.750086e+06  2.739236e+06   2.750086e+06   \n",
      "mean   9.467299e-01  9.507897e-01  7.278536e-02  2.967060e-01   1.303409e-01   \n",
      "std    5.062331e-01  5.083392e-01  9.986519e-02  2.625285e-01   1.996097e-01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00   \n",
      "25%    6.250000e-01  6.250000e-01  0.000000e+00  0.000000e+00   0.000000e+00   \n",
      "50%    8.333333e-01  8.571429e-01  3.333333e-02  2.673381e-01   2.702703e-02   \n",
      "75%    1.200000e+00  1.200000e+00  1.111111e-01  4.727389e-01   1.818182e-01   \n",
      "max    1.000000e+01  9.000000e+00  5.000000e-01  1.000000e+00   1.000000e+00   \n",
      "\n",
      "       diff_stops_r      ...           who_both      q1_where      q2_where  \\\n",
      "count  2.750086e+06      ...       2.750086e+06  2.750086e+06  2.750086e+06   \n",
      "mean  -4.059835e-03      ...       1.067967e-02  2.500795e-02  2.491122e-02   \n",
      "std    6.107030e-01      ...       1.027892e-01  1.561491e-01  1.558546e-01   \n",
      "min   -8.000000e+00      ...       0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%   -3.214286e-01      ...       0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    0.000000e+00      ...       0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%    3.000000e-01      ...       0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "max    8.400000e+00      ...       1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "\n",
      "         where_both       q1_when       q2_when     when_both        q1_why  \\\n",
      "count  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06   \n",
      "mean   7.281590e-03  3.273570e-02  3.223208e-02  7.694668e-03  9.631372e-02   \n",
      "std    8.502101e-02  1.779441e-01  1.766159e-01  8.738114e-02  2.950210e-01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "\n",
      "             q2_why      why_both  \n",
      "count  2.750086e+06  2.750086e+06  \n",
      "mean   9.600136e-02  3.267607e-02  \n",
      "std    2.945932e-01  1.777874e-01  \n",
      "min    0.000000e+00  0.000000e+00  \n",
      "25%    0.000000e+00  0.000000e+00  \n",
      "50%    0.000000e+00  0.000000e+00  \n",
      "75%    0.000000e+00  0.000000e+00  \n",
      "max    1.000000e+00  1.000000e+00  \n",
      "\n",
      "[8 rows x 48 columns]\n",
      "Features: ['word_match', 'word_match_2root', 'tfidf_word_match', 'shared_count', 'stops1_ratio', 'stops2_ratio', 'shared_2gram', 'cosine', 'words_hamming', 'diff_stops_r', 'len_q1', 'len_q2', 'diff_len', 'caps_count_q1', 'caps_count_q2', 'diff_caps', 'len_char_q1', 'len_char_q2', 'diff_len_char', 'len_word_q1', 'len_word_q2', 'diff_len_word', 'avg_world_len1', 'avg_world_len2', 'diff_avg_word', 'exactly_same', 'duplicated', 'q1_how', 'q2_how', 'how_both', 'q1_what', 'q2_what', 'what_both', 'q1_which', 'q2_which', 'which_both', 'q1_who', 'q2_who', 'who_both', 'q1_where', 'q2_where', 'where_both', 'q1_when', 'q2_when', 'when_both', 'q1_why', 'q2_why', 'why_both']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a1ec24d868bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'feats6_whq_jaccard.sav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0mtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.11\n",
    "params['max_depth'] = 5\n",
    "params['silent'] = 1\n",
    "params['seed'] = RS\n",
    "\n",
    "df_train = pd.read_csv(input_folder + 'train.csv')\n",
    "df_test  = pd.read_csv(input_folder + 'test.csv')\n",
    "\t\n",
    "###\n",
    "#\tdf_train['question1'] = df_train['question1'].apply(lambda x:str(x).replace(\"?\",\"\"))\n",
    "#\tdf_train['question2'] = df_train['question2'].apply(lambda x:str(x).replace(\"?\",\"\"))\n",
    "#\tdf_test['question1'] = df_test['question1'].apply(lambda x:str(x).replace(\"?\",\"\"))\n",
    "#\tdf_test['question2'] = df_test['question2'].apply(lambda x:str(x).replace(\"?\",\"\"))\n",
    "###\n",
    "###\t\n",
    "#\tdf_train['question1'] = df_train['question1'].apply(lambda x:str(x).replace(\".\",\"\"))\n",
    "#\tdf_train['question2'] = df_train['question2'].apply(lambda x:str(x).replace(\".\",\"\"))\n",
    "#\tdf_test['question1'] = df_test['question1'].apply(lambda x:str(x).replace(\".\",\"\"))\n",
    "#\tdf_test['question2'] = df_test['question2'].apply(lambda x:str(x).replace(\".\",\"\"))\n",
    "\n",
    "#\tdf_train['question1'] = df_train['question1'].apply(lambda x:str(x).replace(\",\",\"\"))\n",
    "#\tdf_train['question2'] = df_train['question2'].apply(lambda x:str(x).replace(\",\",\"\"))\n",
    "#\tdf_test['question1'] = df_test['question1'].apply(lambda x:str(x).replace(\",\",\"\"))\n",
    "#\tdf_test['question2'] = df_test['question2'].apply(lambda x:str(x).replace(\",\",\"\"))\n",
    "###\n",
    "\t\n",
    "print(\"Original data: X_train: {}, X_test: {}\".format(df_train.shape, df_test.shape))\n",
    "\n",
    "print(\"Features processing, be patient...\")\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "\treturn 0 if count < min_count else 1 / (count + eps)\n",
    "\n",
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def word_shares(row):\n",
    "\tq1_list = str(row['question1']).lower().split()\n",
    "\tq1 = set(q1_list)\n",
    "\tq1words = q1.difference(stops)\n",
    "\tif len(q1words) == 0:\n",
    "\t\treturn '0:0:0:0:0:0:0:0'\n",
    "        \n",
    "\tq2_list = str(row['question2']).lower().split()\n",
    "\tq2 = set(q2_list)\n",
    "\tq2words = q2.difference(stops)\n",
    "\tif len(q2words) == 0:\n",
    "\t\treturn '0:0:0:0:0:0:0:0'\n",
    "\n",
    "\twords_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n",
    "\n",
    "\tq1stops = q1.intersection(stops)\n",
    "\tq2stops = q2.intersection(stops)\n",
    "\n",
    "\tq1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "\tq2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "\n",
    "\tshared_2gram = q1_2gram.intersection(q2_2gram)\n",
    "\n",
    "\tshared_words = q1words.intersection(q2words)\n",
    "\tshared_weights = [weights.get(w, 0) for w in shared_words]\n",
    "\tq1_weights = [weights.get(w, 0) for w in q1words]\n",
    "\tq2_weights = [weights.get(w, 0) for w in q2words]\n",
    "\ttotal_weights = q1_weights + q1_weights\n",
    "\t\n",
    "\tR1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
    "\tR2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n",
    "\tR31 = len(q1stops) / len(q1words) #stops in q1\n",
    "\tR32 = len(q2stops) / len(q2words) #stops in q2\n",
    "\tRcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n",
    "\tRcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n",
    "\tif len(q1_2gram) + len(q2_2gram) == 0:\n",
    "\t\tR2gram = 0\n",
    "\telse:\n",
    "\t\tR2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n",
    "\treturn '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming)\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df['word_shares'] = df.apply(word_shares, axis=1, raw=True)\n",
    "\n",
    "x = pd.DataFrame()\n",
    "\n",
    "x['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "x['word_match_2root'] = np.sqrt(x['word_match'])\n",
    "x['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "x['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "print('25% done....')\n",
    "\n",
    "x['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "x['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "x['shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\n",
    "x['cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\n",
    "x['words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\n",
    "x['diff_stops_r']     = x['stops1_ratio'] - x['stops2_ratio']\n",
    "print('50% done....')\n",
    "\n",
    "x['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
    "x['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
    "x['diff_len'] = x['len_q1'] - x['len_q2']\n",
    "\n",
    "x['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "x['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "x['diff_caps'] = x['caps_count_q1'] - x['caps_count_q2']\n",
    "\n",
    "x['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n",
    "print('75% done....')\n",
    "\n",
    "x['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "x['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "x['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n",
    "\n",
    "x['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\n",
    "x['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\n",
    "x['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n",
    "\n",
    "x['exactly_same'] = (df['question1'] == df['question2']).astype(int)\n",
    "x['duplicated'] = df.duplicated(['question1','question2']).astype(int)\n",
    "print('100% done....')\n",
    "\n",
    "add_word_count(x, df,'how')\n",
    "add_word_count(x, df,'what')\n",
    "add_word_count(x, df,'which')\n",
    "add_word_count(x, df,'who')\n",
    "add_word_count(x, df,'where')\n",
    "add_word_count(x, df,'when')\n",
    "add_word_count(x, df,'why')\n",
    "\n",
    "print(x.columns)\n",
    "print(x.describe())\n",
    "\n",
    "feature_names = list(x.columns.values)\n",
    "create_feature_map(feature_names)\n",
    "print(\"Features: {}\".format(feature_names))\n",
    "\n",
    "x_train = x[:df_train.shape[0]]\n",
    "x_test  = x[df_train.shape[0]:]\n",
    "y_train = df_train['is_duplicate'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Saved\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "'''Saving features'''\n",
    "\n",
    "train_X = np.array(x_train)\n",
    "pickle.dump(train_X, open(input_folder + 'feats6_whq_jaccard.sav', 'wb'))\n",
    "\n",
    "test_X = np.array(x_test)\n",
    "pickle.dump(test_X, open(input_folder + 'feats6_whq_jaccard_for_test.sav', 'wb'))\n",
    "\n",
    "print('Features Saved')\n",
    "del x, df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 48)\n",
      "(2345796, 48)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling started for proportion: 0.369197853026293\n",
      "Oversampling done, new proportion: 0.19124366100096607\n",
      "Training data: X_train: (780486, 48), Y_train: 780486, X_test: (2345796, 48)\n",
      "Will train XGB for 315 rounds, RandomSeed: 12357\n",
      "[0]\ttrain-logloss:0.639288\teval-logloss:0.639319\n",
      "[1]\ttrain-logloss:0.596172\teval-logloss:0.596213\n",
      "[2]\ttrain-logloss:0.560758\teval-logloss:0.560829\n",
      "[3]\ttrain-logloss:0.531422\teval-logloss:0.53154\n",
      "[4]\ttrain-logloss:0.50688\teval-logloss:0.507036\n",
      "[5]\ttrain-logloss:0.485994\teval-logloss:0.486199\n",
      "[6]\ttrain-logloss:0.468151\teval-logloss:0.468385\n",
      "[7]\ttrain-logloss:0.452929\teval-logloss:0.453191\n",
      "[8]\ttrain-logloss:0.439849\teval-logloss:0.440146\n",
      "[9]\ttrain-logloss:0.428434\teval-logloss:0.428778\n",
      "[10]\ttrain-logloss:0.418648\teval-logloss:0.419002\n",
      "[11]\ttrain-logloss:0.410027\teval-logloss:0.410418\n",
      "[12]\ttrain-logloss:0.402565\teval-logloss:0.402989\n",
      "[13]\ttrain-logloss:0.395784\teval-logloss:0.396249\n",
      "[14]\ttrain-logloss:0.389931\teval-logloss:0.390442\n",
      "[15]\ttrain-logloss:0.384603\teval-logloss:0.38518\n",
      "[16]\ttrain-logloss:0.38001\teval-logloss:0.380609\n",
      "[17]\ttrain-logloss:0.376021\teval-logloss:0.376611\n",
      "[18]\ttrain-logloss:0.372333\teval-logloss:0.372983\n",
      "[19]\ttrain-logloss:0.369225\teval-logloss:0.369896\n",
      "[20]\ttrain-logloss:0.366353\teval-logloss:0.367072\n",
      "[21]\ttrain-logloss:0.363916\teval-logloss:0.364675\n",
      "[22]\ttrain-logloss:0.361614\teval-logloss:0.362399\n",
      "[23]\ttrain-logloss:0.359711\teval-logloss:0.360511\n",
      "[24]\ttrain-logloss:0.357898\teval-logloss:0.358715\n",
      "[25]\ttrain-logloss:0.356235\teval-logloss:0.357102\n",
      "[26]\ttrain-logloss:0.354826\teval-logloss:0.355714\n",
      "[27]\ttrain-logloss:0.353617\teval-logloss:0.354552\n",
      "[28]\ttrain-logloss:0.352498\teval-logloss:0.353429\n",
      "[29]\ttrain-logloss:0.351428\teval-logloss:0.352396\n",
      "[30]\ttrain-logloss:0.350512\teval-logloss:0.351488\n",
      "[31]\ttrain-logloss:0.349717\teval-logloss:0.350708\n",
      "[32]\ttrain-logloss:0.349012\teval-logloss:0.350035\n",
      "[33]\ttrain-logloss:0.348377\teval-logloss:0.349428\n",
      "[34]\ttrain-logloss:0.347782\teval-logloss:0.348875\n",
      "[35]\ttrain-logloss:0.347096\teval-logloss:0.348225\n",
      "[36]\ttrain-logloss:0.346497\teval-logloss:0.347623\n",
      "[37]\ttrain-logloss:0.346028\teval-logloss:0.347167\n",
      "[38]\ttrain-logloss:0.34552\teval-logloss:0.346705\n",
      "[39]\ttrain-logloss:0.345048\teval-logloss:0.346222\n",
      "[40]\ttrain-logloss:0.344671\teval-logloss:0.345862\n",
      "[41]\ttrain-logloss:0.344175\teval-logloss:0.345394\n",
      "[42]\ttrain-logloss:0.343781\teval-logloss:0.345015\n",
      "[43]\ttrain-logloss:0.343413\teval-logloss:0.344631\n",
      "[44]\ttrain-logloss:0.343122\teval-logloss:0.34437\n",
      "[45]\ttrain-logloss:0.342848\teval-logloss:0.344114\n",
      "[46]\ttrain-logloss:0.342402\teval-logloss:0.343697\n",
      "[47]\ttrain-logloss:0.342097\teval-logloss:0.343425\n",
      "[48]\ttrain-logloss:0.341808\teval-logloss:0.343153\n",
      "[49]\ttrain-logloss:0.341544\teval-logloss:0.342898\n",
      "[50]\ttrain-logloss:0.341274\teval-logloss:0.342647\n",
      "[51]\ttrain-logloss:0.341037\teval-logloss:0.342452\n",
      "[52]\ttrain-logloss:0.340756\teval-logloss:0.342206\n",
      "[53]\ttrain-logloss:0.340531\teval-logloss:0.342003\n",
      "[54]\ttrain-logloss:0.340304\teval-logloss:0.341776\n",
      "[55]\ttrain-logloss:0.340014\teval-logloss:0.341523\n",
      "[56]\ttrain-logloss:0.339737\teval-logloss:0.341211\n",
      "[57]\ttrain-logloss:0.339446\teval-logloss:0.340942\n",
      "[58]\ttrain-logloss:0.339276\teval-logloss:0.340805\n",
      "[59]\ttrain-logloss:0.339011\teval-logloss:0.340574\n",
      "[60]\ttrain-logloss:0.338856\teval-logloss:0.340443\n",
      "[61]\ttrain-logloss:0.338666\teval-logloss:0.340305\n",
      "[62]\ttrain-logloss:0.338292\teval-logloss:0.33994\n",
      "[63]\ttrain-logloss:0.338151\teval-logloss:0.339823\n",
      "[64]\ttrain-logloss:0.33787\teval-logloss:0.339575\n",
      "[65]\ttrain-logloss:0.3377\teval-logloss:0.33943\n",
      "[66]\ttrain-logloss:0.337449\teval-logloss:0.339232\n",
      "[67]\ttrain-logloss:0.337273\teval-logloss:0.33908\n",
      "[68]\ttrain-logloss:0.337155\teval-logloss:0.338979\n",
      "[69]\ttrain-logloss:0.336966\teval-logloss:0.338786\n",
      "[70]\ttrain-logloss:0.336787\teval-logloss:0.338643\n",
      "[71]\ttrain-logloss:0.336515\teval-logloss:0.338418\n",
      "[72]\ttrain-logloss:0.336248\teval-logloss:0.338145\n",
      "[73]\ttrain-logloss:0.336033\teval-logloss:0.337962\n",
      "[74]\ttrain-logloss:0.335785\teval-logloss:0.337733\n",
      "[75]\ttrain-logloss:0.335695\teval-logloss:0.337646\n",
      "[76]\ttrain-logloss:0.335562\teval-logloss:0.337533\n",
      "[77]\ttrain-logloss:0.335469\teval-logloss:0.337458\n",
      "[78]\ttrain-logloss:0.335206\teval-logloss:0.337203\n",
      "[79]\ttrain-logloss:0.33497\teval-logloss:0.337003\n",
      "[80]\ttrain-logloss:0.334777\teval-logloss:0.336857\n",
      "[81]\ttrain-logloss:0.334679\teval-logloss:0.336767\n",
      "[82]\ttrain-logloss:0.33451\teval-logloss:0.336587\n",
      "[83]\ttrain-logloss:0.334421\teval-logloss:0.336506\n",
      "[84]\ttrain-logloss:0.334278\teval-logloss:0.336385\n",
      "[85]\ttrain-logloss:0.33398\teval-logloss:0.336099\n",
      "[86]\ttrain-logloss:0.33377\teval-logloss:0.335908\n",
      "[87]\ttrain-logloss:0.333647\teval-logloss:0.335823\n",
      "[88]\ttrain-logloss:0.333456\teval-logloss:0.33566\n",
      "[89]\ttrain-logloss:0.333212\teval-logloss:0.335438\n",
      "[90]\ttrain-logloss:0.332995\teval-logloss:0.335258\n",
      "[91]\ttrain-logloss:0.33285\teval-logloss:0.335137\n",
      "[92]\ttrain-logloss:0.33253\teval-logloss:0.33487\n",
      "[93]\ttrain-logloss:0.332383\teval-logloss:0.334725\n",
      "[94]\ttrain-logloss:0.332289\teval-logloss:0.334657\n",
      "[95]\ttrain-logloss:0.332148\teval-logloss:0.334547\n",
      "[96]\ttrain-logloss:0.331959\teval-logloss:0.334386\n",
      "[97]\ttrain-logloss:0.331833\teval-logloss:0.334302\n",
      "[98]\ttrain-logloss:0.331722\teval-logloss:0.334237\n",
      "[99]\ttrain-logloss:0.331525\teval-logloss:0.334072\n",
      "[100]\ttrain-logloss:0.331468\teval-logloss:0.334028\n",
      "[101]\ttrain-logloss:0.331257\teval-logloss:0.333829\n",
      "[102]\ttrain-logloss:0.331109\teval-logloss:0.333718\n",
      "[103]\ttrain-logloss:0.331034\teval-logloss:0.333665\n",
      "[104]\ttrain-logloss:0.330917\teval-logloss:0.333586\n",
      "[105]\ttrain-logloss:0.33068\teval-logloss:0.333392\n",
      "[106]\ttrain-logloss:0.330586\teval-logloss:0.333313\n",
      "[107]\ttrain-logloss:0.330479\teval-logloss:0.333221\n",
      "[108]\ttrain-logloss:0.3304\teval-logloss:0.333159\n",
      "[109]\ttrain-logloss:0.330155\teval-logloss:0.332933\n",
      "[110]\ttrain-logloss:0.330036\teval-logloss:0.332835\n",
      "[111]\ttrain-logloss:0.329897\teval-logloss:0.332717\n",
      "[112]\ttrain-logloss:0.329702\teval-logloss:0.332545\n",
      "[113]\ttrain-logloss:0.329637\teval-logloss:0.332504\n",
      "[114]\ttrain-logloss:0.329494\teval-logloss:0.332376\n",
      "[115]\ttrain-logloss:0.329412\teval-logloss:0.332294\n",
      "[116]\ttrain-logloss:0.329296\teval-logloss:0.332201\n",
      "[117]\ttrain-logloss:0.329147\teval-logloss:0.332082\n",
      "[118]\ttrain-logloss:0.328933\teval-logloss:0.331888\n",
      "[119]\ttrain-logloss:0.328883\teval-logloss:0.331849\n",
      "[120]\ttrain-logloss:0.3288\teval-logloss:0.331786\n",
      "[121]\ttrain-logloss:0.328674\teval-logloss:0.331689\n",
      "[122]\ttrain-logloss:0.328574\teval-logloss:0.331604\n",
      "[123]\ttrain-logloss:0.328492\teval-logloss:0.331523\n",
      "[124]\ttrain-logloss:0.328334\teval-logloss:0.331387\n",
      "[125]\ttrain-logloss:0.328259\teval-logloss:0.331338\n",
      "[126]\ttrain-logloss:0.328095\teval-logloss:0.331215\n",
      "[127]\ttrain-logloss:0.327924\teval-logloss:0.331073\n",
      "[128]\ttrain-logloss:0.32785\teval-logloss:0.33101\n",
      "[129]\ttrain-logloss:0.327772\teval-logloss:0.330968\n",
      "[130]\ttrain-logloss:0.327713\teval-logloss:0.330919\n",
      "[131]\ttrain-logloss:0.327551\teval-logloss:0.330797\n",
      "[132]\ttrain-logloss:0.327498\teval-logloss:0.330761\n",
      "[133]\ttrain-logloss:0.327395\teval-logloss:0.330692\n",
      "[134]\ttrain-logloss:0.327294\teval-logloss:0.330623\n",
      "[135]\ttrain-logloss:0.327066\teval-logloss:0.330421\n",
      "[136]\ttrain-logloss:0.32695\teval-logloss:0.330332\n",
      "[137]\ttrain-logloss:0.326877\teval-logloss:0.330286\n",
      "[138]\ttrain-logloss:0.32682\teval-logloss:0.330225\n",
      "[139]\ttrain-logloss:0.326729\teval-logloss:0.330152\n",
      "[140]\ttrain-logloss:0.326628\teval-logloss:0.330069\n",
      "[141]\ttrain-logloss:0.32653\teval-logloss:0.33001\n",
      "[142]\ttrain-logloss:0.326397\teval-logloss:0.329893\n",
      "[143]\ttrain-logloss:0.326188\teval-logloss:0.329706\n",
      "[144]\ttrain-logloss:0.32614\teval-logloss:0.329666\n",
      "[145]\ttrain-logloss:0.326087\teval-logloss:0.329632\n",
      "[146]\ttrain-logloss:0.325891\teval-logloss:0.329473\n",
      "[147]\ttrain-logloss:0.325861\teval-logloss:0.329457\n",
      "[148]\ttrain-logloss:0.325732\teval-logloss:0.329345\n",
      "[149]\ttrain-logloss:0.325607\teval-logloss:0.32924\n",
      "[150]\ttrain-logloss:0.325517\teval-logloss:0.329183\n",
      "[151]\ttrain-logloss:0.325445\teval-logloss:0.329144\n",
      "[152]\ttrain-logloss:0.325303\teval-logloss:0.329022\n",
      "[153]\ttrain-logloss:0.325214\teval-logloss:0.328947\n",
      "[154]\ttrain-logloss:0.325071\teval-logloss:0.328812\n",
      "[155]\ttrain-logloss:0.325046\teval-logloss:0.328798\n",
      "[156]\ttrain-logloss:0.32502\teval-logloss:0.328778\n",
      "[157]\ttrain-logloss:0.324838\teval-logloss:0.32864\n",
      "[158]\ttrain-logloss:0.324785\teval-logloss:0.328607\n",
      "[159]\ttrain-logloss:0.324681\teval-logloss:0.328513\n",
      "[160]\ttrain-logloss:0.324548\teval-logloss:0.328417\n",
      "[161]\ttrain-logloss:0.324427\teval-logloss:0.328334\n",
      "[162]\ttrain-logloss:0.324401\teval-logloss:0.328317\n",
      "[163]\ttrain-logloss:0.324314\teval-logloss:0.328247\n",
      "[164]\ttrain-logloss:0.324236\teval-logloss:0.328173\n",
      "[165]\ttrain-logloss:0.324072\teval-logloss:0.328051\n",
      "[166]\ttrain-logloss:0.323957\teval-logloss:0.327932\n",
      "[167]\ttrain-logloss:0.323919\teval-logloss:0.327912\n",
      "[168]\ttrain-logloss:0.323719\teval-logloss:0.327726\n",
      "[169]\ttrain-logloss:0.323632\teval-logloss:0.327645\n",
      "[170]\ttrain-logloss:0.32353\teval-logloss:0.327572\n",
      "[171]\ttrain-logloss:0.323478\teval-logloss:0.327528\n",
      "[172]\ttrain-logloss:0.323401\teval-logloss:0.327477\n",
      "[173]\ttrain-logloss:0.323373\teval-logloss:0.327469\n",
      "[174]\ttrain-logloss:0.323353\teval-logloss:0.327461\n",
      "[175]\ttrain-logloss:0.323148\teval-logloss:0.327279\n",
      "[176]\ttrain-logloss:0.323033\teval-logloss:0.327208\n",
      "[177]\ttrain-logloss:0.322968\teval-logloss:0.327173\n",
      "[178]\ttrain-logloss:0.322853\teval-logloss:0.327068\n",
      "[179]\ttrain-logloss:0.322822\teval-logloss:0.32704\n",
      "[180]\ttrain-logloss:0.322686\teval-logloss:0.326931\n",
      "[181]\ttrain-logloss:0.3226\teval-logloss:0.326861\n",
      "[182]\ttrain-logloss:0.322489\teval-logloss:0.326775\n",
      "[183]\ttrain-logloss:0.322311\teval-logloss:0.326624\n",
      "[184]\ttrain-logloss:0.322205\teval-logloss:0.326531\n",
      "[185]\ttrain-logloss:0.322114\teval-logloss:0.326465\n",
      "[186]\ttrain-logloss:0.321963\teval-logloss:0.326357\n",
      "[187]\ttrain-logloss:0.321854\teval-logloss:0.326276\n",
      "[188]\ttrain-logloss:0.32173\teval-logloss:0.326176\n",
      "[189]\ttrain-logloss:0.321654\teval-logloss:0.326122\n",
      "[190]\ttrain-logloss:0.321533\teval-logloss:0.326057\n",
      "[191]\ttrain-logloss:0.3215\teval-logloss:0.326044\n",
      "[192]\ttrain-logloss:0.321486\teval-logloss:0.326035\n",
      "[193]\ttrain-logloss:0.321401\teval-logloss:0.325972\n",
      "[194]\ttrain-logloss:0.321324\teval-logloss:0.325921\n",
      "[195]\ttrain-logloss:0.321306\teval-logloss:0.325911\n",
      "[196]\ttrain-logloss:0.321265\teval-logloss:0.325892\n",
      "[197]\ttrain-logloss:0.321235\teval-logloss:0.325864\n",
      "[198]\ttrain-logloss:0.321116\teval-logloss:0.325767\n",
      "[199]\ttrain-logloss:0.321011\teval-logloss:0.325689\n",
      "[200]\ttrain-logloss:0.320919\teval-logloss:0.325622\n",
      "[201]\ttrain-logloss:0.320894\teval-logloss:0.325607\n",
      "[202]\ttrain-logloss:0.320702\teval-logloss:0.325438\n",
      "[203]\ttrain-logloss:0.320619\teval-logloss:0.325393\n",
      "[204]\ttrain-logloss:0.320466\teval-logloss:0.325258\n",
      "[205]\ttrain-logloss:0.320416\teval-logloss:0.325236\n",
      "[206]\ttrain-logloss:0.320223\teval-logloss:0.325082\n",
      "[207]\ttrain-logloss:0.320152\teval-logloss:0.325036\n",
      "[208]\ttrain-logloss:0.320044\teval-logloss:0.324956\n",
      "[209]\ttrain-logloss:0.319953\teval-logloss:0.324891\n",
      "[210]\ttrain-logloss:0.31983\teval-logloss:0.324801\n",
      "[211]\ttrain-logloss:0.319783\teval-logloss:0.32477\n",
      "[212]\ttrain-logloss:0.319774\teval-logloss:0.32477\n",
      "[213]\ttrain-logloss:0.319748\teval-logloss:0.324744\n",
      "[214]\ttrain-logloss:0.3196\teval-logloss:0.324624\n",
      "[215]\ttrain-logloss:0.319479\teval-logloss:0.324524\n",
      "[216]\ttrain-logloss:0.319387\teval-logloss:0.324452\n",
      "[217]\ttrain-logloss:0.319309\teval-logloss:0.324401\n",
      "[218]\ttrain-logloss:0.319237\teval-logloss:0.324363\n",
      "[219]\ttrain-logloss:0.319193\teval-logloss:0.32434\n",
      "[220]\ttrain-logloss:0.319186\teval-logloss:0.32434\n",
      "[221]\ttrain-logloss:0.319101\teval-logloss:0.324285\n",
      "[222]\ttrain-logloss:0.319019\teval-logloss:0.324209\n",
      "[223]\ttrain-logloss:0.318933\teval-logloss:0.324161\n",
      "[224]\ttrain-logloss:0.318908\teval-logloss:0.324152\n",
      "[225]\ttrain-logloss:0.318885\teval-logloss:0.324142\n",
      "[226]\ttrain-logloss:0.318843\teval-logloss:0.324119\n",
      "[227]\ttrain-logloss:0.318832\teval-logloss:0.32411\n",
      "[228]\ttrain-logloss:0.318765\teval-logloss:0.324079\n",
      "[229]\ttrain-logloss:0.318732\teval-logloss:0.324056\n",
      "[230]\ttrain-logloss:0.318672\teval-logloss:0.324025\n",
      "[231]\ttrain-logloss:0.318549\teval-logloss:0.323917\n",
      "[232]\ttrain-logloss:0.318474\teval-logloss:0.323875\n",
      "[233]\ttrain-logloss:0.31839\teval-logloss:0.323802\n",
      "[234]\ttrain-logloss:0.318278\teval-logloss:0.323706\n",
      "[235]\ttrain-logloss:0.318249\teval-logloss:0.323691\n",
      "[236]\ttrain-logloss:0.318159\teval-logloss:0.323612\n",
      "[237]\ttrain-logloss:0.318131\teval-logloss:0.323589\n",
      "[238]\ttrain-logloss:0.318067\teval-logloss:0.323547\n",
      "[239]\ttrain-logloss:0.317951\teval-logloss:0.323462\n",
      "[240]\ttrain-logloss:0.317872\teval-logloss:0.323404\n",
      "[241]\ttrain-logloss:0.31777\teval-logloss:0.323328\n",
      "[242]\ttrain-logloss:0.317698\teval-logloss:0.32327\n",
      "[243]\ttrain-logloss:0.317615\teval-logloss:0.323224\n",
      "[244]\ttrain-logloss:0.31752\teval-logloss:0.323159\n",
      "[245]\ttrain-logloss:0.317404\teval-logloss:0.323071\n",
      "[246]\ttrain-logloss:0.317369\teval-logloss:0.323051\n",
      "[247]\ttrain-logloss:0.31736\teval-logloss:0.323048\n",
      "[248]\ttrain-logloss:0.317263\teval-logloss:0.322999\n",
      "[249]\ttrain-logloss:0.317239\teval-logloss:0.322989\n",
      "[250]\ttrain-logloss:0.317163\teval-logloss:0.322939\n",
      "[251]\ttrain-logloss:0.317154\teval-logloss:0.322929\n",
      "[252]\ttrain-logloss:0.317088\teval-logloss:0.322882\n",
      "[253]\ttrain-logloss:0.317083\teval-logloss:0.32288\n",
      "[254]\ttrain-logloss:0.316989\teval-logloss:0.322815\n",
      "[255]\ttrain-logloss:0.316947\teval-logloss:0.322794\n",
      "[256]\ttrain-logloss:0.3169\teval-logloss:0.322758\n",
      "[257]\ttrain-logloss:0.316829\teval-logloss:0.322719\n",
      "[258]\ttrain-logloss:0.316784\teval-logloss:0.322686\n",
      "[259]\ttrain-logloss:0.31675\teval-logloss:0.322663\n",
      "[260]\ttrain-logloss:0.316724\teval-logloss:0.322653\n",
      "[261]\ttrain-logloss:0.316706\teval-logloss:0.322643\n",
      "[262]\ttrain-logloss:0.316681\teval-logloss:0.322629\n",
      "[263]\ttrain-logloss:0.316571\teval-logloss:0.322549\n",
      "[264]\ttrain-logloss:0.316485\teval-logloss:0.322489\n",
      "[265]\ttrain-logloss:0.316405\teval-logloss:0.322437\n",
      "[266]\ttrain-logloss:0.316321\teval-logloss:0.322397\n",
      "[267]\ttrain-logloss:0.3163\teval-logloss:0.322386\n",
      "[268]\ttrain-logloss:0.316234\teval-logloss:0.322343\n",
      "[269]\ttrain-logloss:0.316177\teval-logloss:0.322301\n",
      "[270]\ttrain-logloss:0.316125\teval-logloss:0.322274\n",
      "[271]\ttrain-logloss:0.316043\teval-logloss:0.322219\n",
      "[272]\ttrain-logloss:0.315962\teval-logloss:0.322168\n",
      "[273]\ttrain-logloss:0.315881\teval-logloss:0.322119\n",
      "[274]\ttrain-logloss:0.315863\teval-logloss:0.322109\n",
      "[275]\ttrain-logloss:0.315795\teval-logloss:0.322061\n",
      "[276]\ttrain-logloss:0.315756\teval-logloss:0.322028\n",
      "[277]\ttrain-logloss:0.315751\teval-logloss:0.322027\n",
      "[278]\ttrain-logloss:0.315735\teval-logloss:0.322022\n",
      "[279]\ttrain-logloss:0.315722\teval-logloss:0.322016\n",
      "[280]\ttrain-logloss:0.315657\teval-logloss:0.321982\n",
      "[281]\ttrain-logloss:0.315622\teval-logloss:0.321962\n",
      "[282]\ttrain-logloss:0.315601\teval-logloss:0.321952\n",
      "[283]\ttrain-logloss:0.315586\teval-logloss:0.321945\n",
      "[284]\ttrain-logloss:0.31552\teval-logloss:0.321912\n",
      "[285]\ttrain-logloss:0.315431\teval-logloss:0.321858\n",
      "[286]\ttrain-logloss:0.315378\teval-logloss:0.321821\n",
      "[287]\ttrain-logloss:0.315311\teval-logloss:0.321783\n",
      "[288]\ttrain-logloss:0.31527\teval-logloss:0.321757\n",
      "[289]\ttrain-logloss:0.315182\teval-logloss:0.321702\n",
      "[290]\ttrain-logloss:0.315138\teval-logloss:0.321681\n",
      "[291]\ttrain-logloss:0.315073\teval-logloss:0.321638\n",
      "[292]\ttrain-logloss:0.31501\teval-logloss:0.321612\n",
      "[293]\ttrain-logloss:0.31495\teval-logloss:0.32158\n",
      "[294]\ttrain-logloss:0.314886\teval-logloss:0.321539\n",
      "[295]\ttrain-logloss:0.31482\teval-logloss:0.321494\n",
      "[296]\ttrain-logloss:0.314799\teval-logloss:0.321481\n",
      "[297]\ttrain-logloss:0.314717\teval-logloss:0.3214\n",
      "[298]\ttrain-logloss:0.314686\teval-logloss:0.321378\n",
      "[299]\ttrain-logloss:0.314621\teval-logloss:0.321334\n",
      "[300]\ttrain-logloss:0.314512\teval-logloss:0.321249\n",
      "[301]\ttrain-logloss:0.314409\teval-logloss:0.321174\n",
      "[302]\ttrain-logloss:0.314379\teval-logloss:0.321167\n",
      "[303]\ttrain-logloss:0.314336\teval-logloss:0.321144\n",
      "[304]\ttrain-logloss:0.314324\teval-logloss:0.321141\n",
      "[305]\ttrain-logloss:0.314244\teval-logloss:0.321089\n",
      "[306]\ttrain-logloss:0.314166\teval-logloss:0.321041\n",
      "[307]\ttrain-logloss:0.314143\teval-logloss:0.321022\n",
      "[308]\ttrain-logloss:0.314084\teval-logloss:0.320987\n",
      "[309]\ttrain-logloss:0.314047\teval-logloss:0.320981\n",
      "[310]\ttrain-logloss:0.313988\teval-logloss:0.320943\n",
      "[311]\ttrain-logloss:0.313944\teval-logloss:0.320919\n",
      "[312]\ttrain-logloss:0.313916\teval-logloss:0.320899\n",
      "[313]\ttrain-logloss:0.313909\teval-logloss:0.320899\n",
      "[314]\ttrain-logloss:0.313829\teval-logloss:0.320836\n",
      "Writing output...\n",
      "Features importances...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if 1: # Now we oversample the negative class - on your own risk of overfitting!\n",
    "\tpos_train = x_train[y_train == 1]\n",
    "\tneg_train = x_train[y_train == 0]\n",
    "\n",
    "\tprint(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\tp = 0.165\n",
    "\tscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "\twhile scale > 1:\n",
    "\t\tneg_train = pd.concat([neg_train, neg_train])\n",
    "\t\tscale -=1\n",
    "\tneg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "\tprint(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\n",
    "\tx_train = pd.concat([pos_train, neg_train])\n",
    "\ty_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "\tdel pos_train, neg_train\n",
    "\n",
    "print(\"Training data: X_train: {}, Y_train: {}, X_test: {}\".format(x_train.shape, len(y_train), x_test.shape))\n",
    "clr = train_xgb(x_train, y_train, params)\n",
    "preds = predict_xgb(clr, x_test)\n",
    "\n",
    "print(\"Writing output...\")\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = preds *.75\n",
    "sub.to_csv(\"xgb_seed{}_n{}.csv\".format(RS, ROUNDS), index=False)\n",
    "\n",
    "print(\"Features importances...\")\n",
    "importance = clr.get_fscore(fmap='xgb.fmap')\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "ft = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "\n",
    "ft.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\n",
    "plt.gcf().savefig('features_importance.png')\n",
    "\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
